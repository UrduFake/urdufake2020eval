{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Definitions\" data-toc-modified-id=\"Definitions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Definitions</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Precision:-Appropriate-when-minimizing-false-positives-is-the-focus.\" data-toc-modified-id=\"Precision:-Appropriate-when-minimizing-false-positives-is-the-focus.-1.0.0.1\"><span class=\"toc-item-num\">1.0.0.1&nbsp;&nbsp;</span>Precision: Appropriate when minimizing false positives is the focus.</a></span></li><li><span><a href=\"#Recall:-Appropriate-when-minimizing-false-negatives-is-the-focus.\" data-toc-modified-id=\"Recall:-Appropriate-when-minimizing-false-negatives-is-the-focus.-1.0.0.2\"><span class=\"toc-item-num\">1.0.0.2&nbsp;&nbsp;</span>Recall: Appropriate when minimizing false negatives is the focus.</a></span></li></ul></li></ul></li><li><span><a href=\"#Labels-of-Test-Dataset\" data-toc-modified-id=\"Labels-of-Test-Dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Labels of Test Dataset</a></span></li><li><span><a href=\"#Labels-of-Submission\" data-toc-modified-id=\"Labels-of-Submission-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Labels of Submission</a></span></li><li><span><a href=\"#Evaluation-of-the-submitted-results\" data-toc-modified-id=\"Evaluation-of-the-submitted-results-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Evaluation of the submitted results</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "\n",
    "For imbalanced classification problems, the majority class is typically referred to as the negative outcome (e.g. here we have more real news samples), and the minority class is typically referred to as the positive outcome (e.g. here we have less fake news articles).\n",
    "\n",
    "\n",
    "\n",
    "**Precision** quantifies the number of *positive class predictions* that actually belong to the positive class.\n",
    "\n",
    "**Recall** quantifies the number of positive class predictions made out of all positive examples in the dataset.\n",
    "\n",
    "**F-Measure** provides a single score that balances both the concerns of precision and recall in one number.\n",
    "\n",
    "\n",
    "For imbalanced classification problems, the majority class is typically referred to as the negative outcome. and \n",
    "\n",
    "The minority class is typically referred to as the positive outcome.\n",
    "\n",
    "\n",
    "**Our case**:\n",
    "\n",
    "    - Fake news artciles is a positive class (because we have less fake news samples) -- (class 1)\n",
    "    - Real news articles in a negative class (because we have more real samples) -- (class 0)\n",
    "    \n",
    "    \n",
    "**Precision**\n",
    "\n",
    "    1. Precision is a metric that quantifies the number of correct positive predictions made.\n",
    "\n",
    "    2. Precision, therefore, calculates the accuracy for the minority class.\n",
    "\n",
    "    3. It is calculated as the ratio of correctly predicted positive examples divided by the total number of positive \n",
    "       examples that were predicted.\n",
    "    4. Precision = TruePositives / (TruePositives + FalsePositives)\n",
    " \n",
    "##### Precision: Appropriate when minimizing false positives is the focus.\n",
    "    \n",
    "**Recall**  \n",
    "\n",
    "    1.  Recall is a metric that quantifies the number of correct positive predictions made out of all positive predictions \n",
    "        that could have been made.\n",
    "    \n",
    "    2.  Unlike precision that only comments on the correct positive predictions out of all positive predictions, recall        \n",
    "        provides an indication of missed positive predictions.\n",
    "        \n",
    "    3.  Recall provides some notion of the coverage of the positive class.\n",
    "    \n",
    "    4.  Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "    \n",
    "##### Recall: Appropriate when minimizing false negatives is the focus.\n",
    "\n",
    "**F1**  \n",
    "\n",
    "    1. Classification accuracy is widely used because it is one single measure used to summarize model performance.\n",
    "    \n",
    "    2. F-Measure provides a way to combine both precision and recall into a single measure that captures both properties.\n",
    "    \n",
    "    3. F-Measure = (2 * Precision * Recall) / (Precision + Recall)\n",
    "    \n",
    "    4. This is the harmonic mean of the two fractions. This is sometimes called the F-Score or the F1-Score and might be the \n",
    "       most common metric used on imbalanced classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score,accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels of Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File No</th>\n",
       "      <th>Real/Fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   File No Real/Fake\n",
       "0        1         F\n",
       "1        2         F\n",
       "2        3         F\n",
       "3        4         F\n",
       "4        5         F"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig=pd.read_excel('Orignal_labels.xlsx')\n",
    "orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400, 2), Index(['File No', 'Real/Fake'], dtype='object'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig.shape, orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File No</th>\n",
       "      <th>Real/Fake</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   File No Real/Fake  label\n",
       "0        1         F      1\n",
       "1        2         F      1\n",
       "2        3         F      1\n",
       "3        4         F      1\n",
       "4        5         F      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Added a new column to make labels in Numeric form, like F --> 1, R --> 0\n",
    "\n",
    "orig['label'] = orig['Real/Fake'].apply(lambda x: 1 if x == 'F' else 0)\n",
    "orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels of Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File No</th>\n",
       "      <th>Real/Fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>278</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>275</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   File No Real/Fake\n",
       "0       44         R\n",
       "1      278         F\n",
       "2      276         R\n",
       "3       84         F\n",
       "4      275         F"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=pd.read_csv('Cyber Pilots Run 2.csv')\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File No</th>\n",
       "      <th>Real/Fake</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>3</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>4</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>6</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>6</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     File No Real/Fake  label\n",
       "370        3         R      0\n",
       "328        3         F      1\n",
       "339        4         R      0\n",
       "208        6         F      1\n",
       "103        6         R      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Added a new column to make labels in Numeric form, like F --> 1, R --> 0\n",
    "pred['label'] = pred['Real/Fake'].apply(lambda x: 1 if x == 'F' else 0)\n",
    "\n",
    "pred= pred.sort_values(by='File No')\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the submitted results\n",
    "    -Precision\n",
    "    -Recall\n",
    "    -Accuracy\n",
    "    -F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[118, 132],\n",
       "       [ 70,  80]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(orig['label'], pred['label'])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 132, 70, 80)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Fake class : 0.37735849056603776\n",
      "Recall for Fake class : 0.5333333333333333\n",
      "F1_Fake : 0.44198895027624313\n",
      "\n",
      "Precision for Real class : 0.6276595744680851\n",
      "Recall for Real class : 0.472\n",
      "F1_Real : 0.5388127853881278\n",
      "\n",
      "F1_Macro : 0.4904008678321855\n",
      "The Accuracy score is:  0.495\n",
      "\n",
      "F1_Average : 0.5025038472211711\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "###################For Fake Class######################\n",
    "\n",
    "prec_fake = tp/(tp + fp)\n",
    "print('Precision for Fake class :', prec_fake)\n",
    "\n",
    "\n",
    "rec_fake = tp/(tp + fn)\n",
    "print('Recall for Fake class :', rec_fake)\n",
    "\n",
    "\n",
    "f1_fake = 2 * prec_fake * rec_fake / ( prec_fake + rec_fake)\n",
    "print('F1_Fake :', f1_fake)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "###################For Real Class######################\n",
    "prec_real = tn/(tn + fn)\n",
    "print('\\nPrecision for Real class :', prec_real)\n",
    "\n",
    "\n",
    "rec_real = tn/(tn + fp)\n",
    "print('Recall for Real class :', rec_real)\n",
    "\n",
    "\n",
    "f1_real = 2 * prec_real * rec_real / ( prec_real + rec_real)\n",
    "print('F1_Real :', f1_real)\n",
    "\n",
    "\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "f1_mac = (f1_real + f1_fake )/2\n",
    "print('\\nF1_Macro :', f1_mac)\n",
    "print('The Accuracy score is: ', accuracy_score(orig['label'], pred['label']))\n",
    "\n",
    "\n",
    "#Calculate metrics (f1 ) for each label, \n",
    "#and find their average weighted by support (the number of true instances for each label).\n",
    "#This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "f1_weighted = (250 /400) * f1_real + (150 /400) * f1_fake\n",
    "print('\\nF1_Average :', f1_weighted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
