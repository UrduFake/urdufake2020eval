{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Fake-news-detection\" data-toc-modified-id=\"Fake-news-detection-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Fake news detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-libraries\" data-toc-modified-id=\"Importing-libraries-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Importing libraries</a></span></li><li><span><a href=\"#Feature-extraction-functions\" data-toc-modified-id=\"Feature-extraction-functions-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Feature extraction functions</a></span></li><li><span><a href=\"#Reading-and-preparing-the-corpus\" data-toc-modified-id=\"Reading-and-preparing-the-corpus-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Reading and preparing the corpus</a></span></li><li><span><a href=\"#Parametrization-and-feature-extraction\" data-toc-modified-id=\"Parametrization-and-feature-extraction-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Parametrization and feature extraction</a></span></li><li><span><a href=\"#Frequency-threshold\" data-toc-modified-id=\"Frequency-threshold-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Frequency threshold</a></span></li><li><span><a href=\"#Weighting-schemes\" data-toc-modified-id=\"Weighting-schemes-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Weighting schemes</a></span></li><li><span><a href=\"#Classification-Process\" data-toc-modified-id=\"Classification-Process-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Classification Process</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initializing-classification-algorithms\" data-toc-modified-id=\"Initializing-classification-algorithms-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>Initializing classification algorithms</a></span></li><li><span><a href=\"#Cross-Validation-Classification-and-Evaluation-Function\" data-toc-modified-id=\"Cross-Validation-Classification-and-Evaluation-Function-1.8.2\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>Cross-Validation Classification and Evaluation Function</a></span></li></ul></li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-the-performance-of-Character-bi-gram-Baseline\" data-toc-modified-id=\"Test-the-performance-of-Character-bi-gram-Baseline-1.9.1\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;</span>Test the performance of Character bi-gram Baseline</a></span></li></ul></li><li><span><a href=\"#Evaluation-Matrix\" data-toc-modified-id=\"Evaluation-Matrix-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Evaluation Matrix</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake news detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "import string\n",
    "import codecs\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from string import punctuation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from random import randrange\n",
    "from scipy.sparse import csr_matrix, csc_matrix, hstack, coo_matrix\n",
    "from gensim.matutils import Scipy2Corpus, corpus2csc\n",
    "from gensim.models.logentropy_model import LogEntropyModel\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, make_scorer, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create a list of required libraries and their versions \n",
    "#!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts word-ngrams, when n=1 is equal to bag of words\n",
    "def wordNgrams(text, n):\n",
    "    ngrams = []\n",
    "    text = [word for word in text.split() if word not in string.punctuation]\n",
    "    ngrams = [' '.join(text[i:i+n])+'' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts character n-grams\n",
    "def charNgrams(text, n):\n",
    "    ngrams = []\n",
    "    ngrams = [text[i:i+n]+'_cng' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diccionario(ruta):\n",
    "    terms = set()#Dictionary of slangs\n",
    "    try:\n",
    "        tmp = open(ruta, \"r\", encoding=\"utf8\" )     \n",
    "        while True :\n",
    "            linea = tmp.readline()                                                                                   \n",
    "            #linea = to_unicode(linea) \n",
    "            if (not linea) or (linea == \"\"):                                                                               \n",
    "                break;                                                                                                      \n",
    "            linea = linea.rstrip()\n",
    "            terms.add(linea.lower())\n",
    "        return (terms)\n",
    "    except IOError as e:\n",
    "        print (\"Error: \"+ruta+\" I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts function words n-grams with a pre-loaded dictionary\n",
    "def funcNgrams(text, n):\n",
    "    stop_words = load_diccionario('stop_words.txt')\n",
    "    patt=r'\\b(' + ('|'.join(re.escape(key) for key in stop_words)).lstrip('|') + r')\\b'\n",
    "    pattern = re.compile(patt)\n",
    "    text = re.sub(r\"(\\n+|\\r+|(\\r\\n)+)\", \" \", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(r\"[\" + punctuation + \"]*\", \"\", text)\n",
    "    terms = pattern.findall(text)\n",
    "    n_grams=[('_'.join(terms[i:i+n])) + \"_fwn\" for i in range(len(terms)-n+1)]\n",
    "\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text,cn,wn,fn):\n",
    "    text = text.lower()\n",
    "    #text=clean_text(text)\n",
    "    features = []\n",
    "    for n in wn:\n",
    "        if n != 0:\n",
    "            features.extend(wordNgrams(text,n))\n",
    "    for n in cn:\n",
    "        if n != 0:\n",
    "            features.extend(charNgrams(text,n))\n",
    "    for n in fn:\n",
    "            if n != 0:\n",
    "                features.extend(funcNgrams(text,n))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts all features in a set of 'texts' and return as a string separated with the simbol '&%$'\n",
    "def process_texts(texts,cn,wn,fn):\n",
    "    occurrences=defaultdict(int)\n",
    "    featuresList=[]\n",
    "    featuresDict=Counter()\n",
    "    for (text) in texts:\n",
    "        features=extract_features(text,cn,wn,fn)\n",
    "        featuresDict.update(features)\n",
    "        featuresList.append('&%$'.join(features))\n",
    "    return featuresList, featuresDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and preparing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessText(text):\n",
    "    cleantext = re.sub('\\d','0',text)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to remove stop words, we should use this (below) function 1. Anyhow we need to use either function 1 or function 2. we can't use both at the same time, it means we need to comment one of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1\n",
    "# utility function for reading files\n",
    "def read_txt_files(files):\n",
    "    text=[]\n",
    "    topic=[]\n",
    "    for i,file_path in sorted(enumerate(files)):\n",
    "        #print(file_path.split('\\\\')[1].split('.')[0])\n",
    "        with open(file_path,\"r\", encoding=\"utf8\") as infile:\n",
    "            cleantext = preprocessText(infile.read())\n",
    "            text.append(cleantext)\n",
    "            file_topic=''.join(re.findall('[A-Za-z0-9]',file_path.split('\\\\')[1].split('.')[0]))\n",
    "            topic.append(file_topic)\n",
    "    return text, topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do not want to remove stop words. I should use this (below) function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function 2\n",
    "# #utility function for reading files\n",
    "# def read_txt_files(files):\n",
    "#     text=[]\n",
    "#     topic=[]\n",
    "#     for i,file_path in sorted(enumerate(files)):\n",
    "#         print('read', file_path)\n",
    "#         with open(file_path,\"r\", encoding=\"utf8\") as infile:\n",
    "#             text.append(infile.read())\n",
    "#             file_topic=''.join(re.findall('[A-Za-z]',file_path.split('\\\\')[3].split('.')[0]))\n",
    "#             topic.append(file_topic)\n",
    "#     return text, topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the path of real and fake news for training\n",
    "train_path_real='Data\\\\Train\\\\Real\\\\'\n",
    "train_path_fake='Data\\\\Train\\\\Fake\\\\'\n",
    "\n",
    "real_news, real_news_topics = read_txt_files(sorted(glob.glob(train_path_real+'*.txt')))\n",
    "\n",
    "fake_news, fake_news_topics = read_txt_files(sorted(glob.glob(train_path_fake+'*.txt')))\n",
    "\n",
    "#contatenating real and fake news in one variable for training\n",
    "train_texts = np.concatenate((real_news, fake_news))\n",
    "train_labels = np.concatenate((np.zeros(len(real_news)), np.ones(len(fake_news))))\n",
    "train_topics = np.concatenate((real_news_topics, fake_news_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\t Real: 500\n",
      "\t Fake: 400\n"
     ]
    }
   ],
   "source": [
    "print ('Train:')\n",
    "print ('\\t Real:',len(real_news))\n",
    "print ('\\t Fake:',len(fake_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the path of real and fake news for testing\n",
    "test_path_real='Data\\\\Test\\\\Real\\\\'\n",
    "test_path_fake='Data\\\\Test\\\\Fake\\\\'\n",
    "\n",
    "real_news, real_news_topics = read_txt_files(sorted(glob.glob(test_path_real+'*.txt')))\n",
    "fake_news, fake_news_topics = read_txt_files(sorted(glob.glob(test_path_fake+'*.txt')))\n",
    "\n",
    "#contatenating real and fake news in one variable for testing\n",
    "test_texts = np.concatenate((real_news, fake_news))\n",
    "test_labels = np.concatenate((np.zeros(len(real_news)), np.ones(len(fake_news))))\n",
    "test_topics = np.concatenate((real_news_topics, fake_news_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:\n",
      "\t Real: 250\n",
      "\t Fake: 150\n"
     ]
    }
   ],
   "source": [
    "print ('Test:')\n",
    "print ('\\t Real:',len(real_news))\n",
    "print ('\\t Fake:',len(fake_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrization and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "cnvalues=[2] #character n-grams\n",
    "wnvalues=[0] # word n-grams; bag of words\n",
    "fnvalues=[0] # function words n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features\n",
      "\t Labels for each document:  900\n",
      "\t Total training files (Real + Fake) :  900\n",
      "\t Vocabulary size of 900 files is :  2800\n",
      "\t Train shape: (900, 2312)\n",
      "\t class dictribution Counter({0.0: 500, 1.0: 400})\n"
     ]
    }
   ],
   "source": [
    "#Train feature extraction\n",
    "print('Extracting features')\n",
    "train_features, dicOfFeatures = process_texts(train_texts,cnvalues,wnvalues,fnvalues)\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False, min_df=2, tokenizer=lambda x: x.split('&%$')) #--> we can change this\n",
    "train_data = vectorizer.fit_transform(train_features)\n",
    "train_data = train_data.astype(float)\n",
    "print('\\t', 'Labels for each document: ', len(train_labels))\n",
    "print('\\t', 'Total training files (Real + Fake) : ', len(train_texts))\n",
    "print('\\t', 'Vocabulary size of', len(train_texts), 'files is : ',len(dicOfFeatures))\n",
    "print ('\\t','Train shape:',train_data.shape)\n",
    "print('\\t', 'class dictribution',Counter(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels for each documents means that we have total 900 files in which first 500 files are assigned value 0 (we assign value 0 if a documents belongs to real class, and we assigned value one if a documents belongs to fake class), and rest 288 files are assigned value one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Test features\n",
      "\t Total testing files (Real + Fake):  400\n",
      "\t vocabulary size:  2325\n",
      "\t Test shape: (400, 2312)\n",
      "\t class dictribution:  Counter({0.0: 250, 1.0: 150})\n"
     ]
    }
   ],
   "source": [
    "# Test feature extraction\n",
    "print('Extracting Test features')\n",
    "test_features,dicOfFeaturesTest = process_texts(test_texts,cnvalues,wnvalues,fnvalues)\n",
    "test_data = vectorizer.transform(test_features)\n",
    "test_data = test_data.astype(float)\n",
    "\n",
    "print('\\t', 'Total testing files (Real + Fake): ', len(test_texts))\n",
    "print('\\t', 'vocabulary size: ',len(dicOfFeaturesTest))\n",
    "print ('\\t','Test shape:',test_data.shape)\n",
    "print('\\t', 'class dictribution: ',Counter(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 5 means we remove all the words from the train data that has frequency less than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (900, 1803)\n",
      "Test shape: (400, 2312)\n"
     ]
    }
   ],
   "source": [
    "N=5\n",
    "X=train_data\n",
    "values=np.array(X.sum(axis=0)).ravel()\n",
    "thresholdMask=(values >= N)*1\n",
    "indices_zero = list(np.nonzero(thresholdMask == 0)[0])\n",
    "all_cols = np.arange(X.shape[1])\n",
    "cols_to_keep = np.where(np.logical_not(np.in1d(all_cols, indices_zero)))[0]\n",
    "train_data = X[:, cols_to_keep]\n",
    "#####\n",
    "\n",
    "scaled_train_data=train_data\n",
    "scaled_test_data=test_data\n",
    "print('Train shape:',scaled_train_data.shape)\n",
    "print('Test shape:',scaled_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perfrom the experiments, the dimensions of testing and training features should\n",
    "be the same. In order to see the testing features, the below cell will perform this \n",
    "task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (400, 1803)\n"
     ]
    }
   ],
   "source": [
    "Z=test_data\n",
    "all_cols = np.arange(Z.shape[1])\n",
    "cols_to_keep = np.where(np.logical_not(np.in1d(all_cols, indices_zero)))[0]\n",
    "test_data = Z[:, cols_to_keep]\n",
    "scaled_test_data=test_data\n",
    "print('Test shape:',scaled_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_weight = tfidf\n"
     ]
    }
   ],
   "source": [
    "# print ('only frecuency:',test_data)\n",
    "feature_weight='tfidf' # possible values: binary, logent, tfidf, norm, relat\n",
    "\n",
    "if feature_weight == 'binary':\n",
    "    scaled_train_data = preprocessing.Binarizer().fit_transform(scaled_train_data)\n",
    "    scaled_test_data = preprocessing.Binarizer().fit_transform(scaled_test_data)\n",
    "    print (\"feature_weight = binary\")\n",
    "    \n",
    "elif feature_weight == 'logent':\n",
    "    Xc = Scipy2Corpus(scaled_train_data)\n",
    "    log_ent = LogEntropyModel(Xc)\n",
    "    X = log_ent[Xc]\n",
    "    X = corpus2csc(X)\n",
    "    scaled_train_data = sp.csc_matrix.transpose(X)\n",
    "    \n",
    "    Xtest = Scipy2Corpus(scaled_test_data)\n",
    "    X = log_ent[Xtest]\n",
    "    X = corpus2csc(X, scaled_train_data.shape[1])\n",
    "    scaled_test_data = sp.csc_matrix.transpose(X)\n",
    "    print (\"feature_weight = logent\")\n",
    "    \n",
    "elif feature_weight == 'tfidf':\n",
    "    transformer = TfidfTransformer()\n",
    "    scaled_train_data = transformer.fit_transform(scaled_train_data)\n",
    "    scaled_test_data = transformer.transform(scaled_test_data)\n",
    "    print (\"feature_weight = tfidf\")\n",
    "    \n",
    "elif feature_weight=='norm':\n",
    "    scaled_train_data = preprocessing.normalize(scaled_train_data, norm='l2')\n",
    "    max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "    scaled_train_data = max_abs_scaler.fit_transform(scaled_train_data)\n",
    "    scaled_test_data = max_abs_scaler.transform(scaled_test_data)\n",
    "    print (\"feature_weight = norm\")\n",
    "    \n",
    "elif feature_weight=='relat':\n",
    "    s = scaled_train_data.sum(axis = 1)\n",
    "    scaled_train_data = coo_matrix(np.nan_to_num(scaled_train_data/s))\n",
    "    s = scaled_test_data.sum(axis = 1)\n",
    "    scaled_test_data = coo_matrix(np.nan_to_num(scaled_test_data/s))\n",
    "    print (\"feature_weight = relat\")\n",
    "    \n",
    "else:\n",
    "    print (\"feature_weight = tf\")\n",
    "    \n",
    "# print ('with weighting scheme:',scaled_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying classification algorithms\n",
    "clf=LinearSVC(C=0.01,class_weight='balanced', random_state=85)\n",
    "clfSVC=SVC(C=0.01, kernel='linear',class_weight='balanced')\n",
    "clfSVC.fit(scaled_train_data, train_labels)\n",
    "clfMnb=MultinomialNB()\n",
    "clfMnb.fit(scaled_train_data, train_labels)\n",
    "clfBnb=BernoulliNB()\n",
    "clfBnb.fit(scaled_train_data, train_labels)\n",
    "clfLG=LogisticRegression(solver='lbfgs', tol=0.001, C=0.01,class_weight='balanced')\n",
    "clfLG.fit(scaled_train_data, train_labels)\n",
    "clfDT=DecisionTreeClassifier(random_state=0)\n",
    "clfDT.fit(scaled_train_data, train_labels)\n",
    "clfRFC=RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "clfRFC.fit(scaled_train_data, train_labels)\n",
    "clfAB=AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0,  random_state=None)\n",
    "clfAB.fit(scaled_train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Classification and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Utility function\n",
    "originalclass=[]\n",
    "predictedclass=[]\n",
    "def classification_report_with_f1_score(y_true, y_pred):\n",
    "    originalclass.extend(y_true)\n",
    "    predictedclass.extend(y_pred)\n",
    "    return f1_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier\n"
     ]
    }
   ],
   "source": [
    "print('Training Classifier')\n",
    "    \n",
    "\n",
    "\n",
    "nested_score = cross_val_score(clf, X=scaled_train_data, y=train_labels, cv=10, scoring=make_scorer(classification_report_with_f1_score))\n",
    "#cvScoreLinearSVC=cross_val_score(clf, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "\n",
    "cvScoreMnb=cross_val_score(clfMnb, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "#print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "\n",
    "\n",
    "cvScoreSVC=cross_val_score(clfSVC, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "#print('10-Fold Cross-validation Linear SVC',cvScoreSVC)\n",
    "\n",
    "cvScoreLG=cross_val_score(clfLG, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "#print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "\n",
    "cvScoreAD=cross_val_score(clfAB, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "#print('10-Fold Cross-validation AdaBoost',cvScoreAD)\n",
    "\n",
    "cvScoreDT=cross_val_score(clfDT, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "#print('10-Fold Cross-validation Decision Tree',cvScoreDT)\n",
    "\n",
    "cvScoreRFC=cross_val_score(clfRFC, scaled_train_data, train_labels, cv=10, scoring='f1').mean()\n",
    "#print('10-Fold Cross-validation Random Forest',cvScoreRFC)\n",
    "\n",
    "# print('10-Fold Cross-validation Linear SVC',nested_score.mean())\n",
    "#print(classification_report(originalclass, predictedclass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the performance of Character bi-gram Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      1.00      0.83       250\n",
      "         1.0       1.00      0.30      0.46       150\n",
      "\n",
      "    accuracy                           0.74       400\n",
      "   macro avg       0.85      0.65      0.64       400\n",
      "weighted avg       0.82      0.74      0.69       400\n",
      "\n",
      "\n",
      " MultiomialNaviev Bayes\n",
      "Accuracy 0.7375\n",
      "F1-score 0.4615384615384615\n",
      "F1-mac 0.6439923712650985\n",
      "F1-weighted 0.6896058486967577\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes\n",
    "prediction_MNB=clfMnb.predict(scaled_test_data)\n",
    "print(classification_report(test_labels, prediction_MNB))\n",
    "print('\\n MultiomialNaviev Bayes')\n",
    "print('Accuracy',accuracy_score(test_labels, prediction_MNB))\n",
    "print('F1-score',f1_score(test_labels, prediction_MNB))\n",
    "print('F1-mac',f1_score(test_labels, prediction_MNB, average='macro'))\n",
    "print('F1-weighted',f1_score(test_labels, prediction_MNB, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      1.00      0.77       250\n",
      "         1.0       0.00      0.00      0.00       150\n",
      "\n",
      "    accuracy                           0.62       400\n",
      "   macro avg       0.31      0.50      0.38       400\n",
      "weighted avg       0.39      0.62      0.48       400\n",
      "\n",
      "\n",
      " SVC\n",
      "Accuracy 0.625\n",
      "F1-score 0.0\n",
      "F1-mac 0.38461538461538464\n",
      "F1-weighted 0.4807692307692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "prediction_SVC=clfSVC.predict(scaled_test_data)\n",
    "print(classification_report(test_labels, prediction_SVC))\n",
    "print('\\n SVC')\n",
    "print('Accuracy',accuracy_score(test_labels, prediction_SVC))\n",
    "print('F1-score',f1_score(test_labels, prediction_SVC))\n",
    "print('F1-mac',f1_score(test_labels, prediction_SVC, average='macro'))\n",
    "print('F1-weighted',f1_score(test_labels, prediction_SVC, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.64      0.77       250\n",
      "         1.0       0.62      0.95      0.75       150\n",
      "\n",
      "    accuracy                           0.76       400\n",
      "   macro avg       0.79      0.80      0.76       400\n",
      "weighted avg       0.83      0.76      0.76       400\n",
      "\n",
      "\n",
      " Bernalo Naviev Bayes\n",
      "Accuracy 0.76\n",
      "F1-score 0.7486910994764399\n",
      "F1-mac 0.7595130138530526\n",
      "F1-weighted 0.7622184924472057\n"
     ]
    }
   ],
   "source": [
    "prediction_Bnb=clfBnb.predict(scaled_test_data)\n",
    "print(classification_report(test_labels, prediction_Bnb))\n",
    "print('\\n Bernalo Naviev Bayes')\n",
    "print('Accuracy',accuracy_score(test_labels, prediction_Bnb))\n",
    "print('F1-score',f1_score(test_labels, prediction_Bnb))\n",
    "print('F1-mac',f1_score(test_labels, prediction_Bnb, average='macro'))\n",
    "print('F1-weighted',f1_score(test_labels, prediction_Bnb, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.89      0.91       250\n",
      "         1.0       0.83      0.90      0.87       150\n",
      "\n",
      "    accuracy                           0.90       400\n",
      "   macro avg       0.89      0.90      0.89       400\n",
      "weighted avg       0.90      0.90      0.90       400\n",
      "\n",
      "\n",
      " Logistic Regression\n",
      "Accuracy 0.895\n",
      "F1-score 0.8653846153846153\n",
      "F1-mac 0.8896595208070617\n",
      "F1-weighted 0.8957282471626733\n"
     ]
    }
   ],
   "source": [
    "prediction_LG=clfLG.predict(scaled_test_data)\n",
    "print(classification_report(test_labels, prediction_LG))\n",
    "print('\\n Logistic Regression')\n",
    "print('Accuracy',accuracy_score(test_labels, prediction_LG))\n",
    "print('F1-score',f1_score(test_labels, prediction_LG))\n",
    "print('F1-mac',f1_score(test_labels, prediction_LG, average='macro'))\n",
    "print('F1-weighted',f1_score(test_labels, prediction_LG, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83       250\n",
      "         1.0       0.72      0.73      0.72       150\n",
      "\n",
      "    accuracy                           0.79       400\n",
      "   macro avg       0.78      0.78      0.78       400\n",
      "weighted avg       0.79      0.79      0.79       400\n",
      "\n",
      "\n",
      " Decision Tree\n",
      "Accuracy 0.79\n",
      "F1-score 0.7218543046357616\n",
      "F1-mac 0.7765898029202905\n",
      "F1-weighted 0.7902736774914225\n"
     ]
    }
   ],
   "source": [
    "prediction_DT=clfDT.predict(scaled_test_data)\n",
    "print(classification_report(test_labels, prediction_DT))\n",
    "print('\\n Decision Tree')\n",
    "print('Accuracy',accuracy_score(test_labels, prediction_DT))\n",
    "print('F1-score',f1_score(test_labels, prediction_DT))\n",
    "print('F1-mac',f1_score(test_labels, prediction_DT, average='macro'))\n",
    "print('F1-weighted',f1_score(test_labels, prediction_DT, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      1.00      0.77       250\n",
      "         1.0       0.00      0.00      0.00       150\n",
      "\n",
      "    accuracy                           0.62       400\n",
      "   macro avg       0.31      0.50      0.38       400\n",
      "weighted avg       0.39      0.62      0.48       400\n",
      "\n",
      "\n",
      " Random Forest\n",
      "Accuracy 0.625\n",
      "F1-score 0.0\n",
      "F1-mac 0.38461538461538464\n",
      "F1-weighted 0.4807692307692308\n"
     ]
    }
   ],
   "source": [
    "prediction_RFC=clfRFC.predict(scaled_test_data)\n",
    "print(classification_report(test_labels, prediction_RFC))\n",
    "print('\\n Random Forest')\n",
    "print('Accuracy',accuracy_score(test_labels, prediction_RFC))\n",
    "print('F1-score',f1_score(test_labels, prediction_RFC))\n",
    "print('F1-mac',f1_score(test_labels, prediction_RFC, average='macro'))\n",
    "print('F1-weighted',f1_score(test_labels, prediction_RFC, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.96      0.84       250\n",
      "         1.0       0.86      0.45      0.59       150\n",
      "\n",
      "    accuracy                           0.77       400\n",
      "   macro avg       0.80      0.70      0.72       400\n",
      "weighted avg       0.79      0.77      0.75       400\n",
      "\n",
      "\n",
      " Adaboost\n",
      "Accuracy 0.7675\n",
      "F1-score 0.593886462882096\n",
      "F1-mac 0.7155071543832546\n",
      "F1-weighted 0.7459123272585443\n"
     ]
    }
   ],
   "source": [
    "prediction_AB=clfAB.predict(scaled_test_data)\n",
    "print(classification_report(test_labels, prediction_AB))\n",
    "print('\\n Adaboost')\n",
    "print('Accuracy',accuracy_score(test_labels, prediction_AB))\n",
    "print('F1-score',f1_score(test_labels, prediction_AB))\n",
    "print('F1-mac',f1_score(test_labels, prediction_AB, average='macro'))\n",
    "print('F1-weighted',f1_score(test_labels, prediction_AB, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the Logistic Regression obtained the best results. Therefore, we are only calculating the \n",
    "    1. Precision, \n",
    "    2. Recall, \n",
    "    3. F1_Real, \n",
    "    4. F1_Fake\n",
    "    5. Accuracy\n",
    "    6. F1-macro \n",
    "    \n",
    "for Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Matrix\n",
    "\n",
    "    1. Precision = TP \\ TP + FP\n",
    "    2. Recall = TP \\TP + FN \n",
    "    3. Precision X Recall \\(Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[223,  27],\n",
       "       [ 15, 135]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(test_labels, prediction_LG)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223, 27, 15, 135)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Fake class : 0.8333333333333334\n",
      "Recall for Fake class : 0.9\n",
      "F1_Fake : 0.8653846153846153\n",
      "\n",
      "Precision for Real class : 0.9369747899159664\n",
      "Recall for Real class : 0.892\n",
      "F1_Real : 0.9139344262295082\n",
      "\n",
      "F1_Macro : 0.8896595208070617\n",
      "\n",
      "F1_Average : 0.8957282471626733\n"
     ]
    }
   ],
   "source": [
    "prec_fake = tp/(tp + fp)\n",
    "print('Precision for Fake class :', prec_fake)\n",
    "\n",
    "\n",
    "rec_fake = tp/(tp + fn)\n",
    "print('Recall for Fake class :', rec_fake)\n",
    "\n",
    "\n",
    "f1_fake = 2 * prec_fake * rec_fake / ( prec_fake + rec_fake)\n",
    "print('F1_Fake :', f1_fake)\n",
    "\n",
    "\n",
    "\n",
    "prec_real = tn/(tn + fn)\n",
    "print('\\nPrecision for Real class :', prec_real)\n",
    "\n",
    "\n",
    "\n",
    "rec_real = tn/(tn + fp)\n",
    "print('Recall for Real class :', rec_real)\n",
    "\n",
    "\n",
    "f1_real = 2 * prec_real * rec_real / ( prec_real + rec_real)\n",
    "print('F1_Real :', f1_real)\n",
    "\n",
    "\n",
    "f1_mac = (f1_real + f1_fake )/2\n",
    "print('\\nF1_Macro :', f1_mac)\n",
    "\n",
    "\n",
    "\n",
    "#Calculate metrics (f1 ) for each label, \n",
    "#and find their average weighted by support (the number of true instances for each label).\n",
    "#This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "f1_weighted = (250 /400) * f1_real + (150 /400) * f1_fake\n",
    "print('\\nF1_Average :', f1_weighted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
